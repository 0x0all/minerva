{"name":"Minerva","tagline":"A redesigned Minerva. It provides matrix programming interface, just like MATLAB. Python bindings and C++ bindings are both available. The resulting code can be run on CPU or GPU. Multi-GPU support is very easy. Please refer to the examples to see how multi-GPU setting is used.","body":"# Minerva: a fast and flexible system for deep learning\r\n\r\n## Goal\r\n\r\nMake deep learning a home dish.\r\n\r\n## Features\r\n\r\n* Matrix programming interface\r\n* Easy interaction with NumPy\r\n* Multi-GPU, multi-CPU support\r\n* Good performance: ImageNet training achieves 213 images/s with one Titan GPU, 403Images/s with two GPUs\r\n\r\n## Installation\r\n\r\n### Requirements and Dependencies\r\n\r\nMinerva depends on several packages. If they are not in the system search path, please set variable `EXTERN_INCLUDE_PATH` and `EXTERN_LIB_PATH` in `configure.in` accordingly.\r\n\r\n* CUDA 6.5\r\n* cuDNN\r\n* LMDB\r\n* NumPy\r\n\r\nIn addition, Minerva also depends on the following packages. A script `resolve_deps` is provided for automatic resolving of them. Running this script will download and install them in the `deps` directory relative to the root of the repository. If you do so, please consult `configure.in.example` for a sample configuration of the paths.\r\n\r\n* Boost\r\n* Boost.NumPy\r\n* glog\r\n* gflags\r\n* Google Test\r\n\r\n### How to build\r\n\r\n1. Set `CXX_COMPILER`, and `C_COMPILER` in `configure.in`. `g++-4.8` is strongly recommended.\r\n1. Set `EXTERN_INCLUDE_PATH`, and `EXTERN_LIB_PATH` in `configure.in` to reflect any dependency not in the system search path. If there are more than one, use a comma-separated list.\r\n1. Run `./configure`.\r\n1. Change directory into `release` (or `debug` for more verbose logging) and run `make`.\r\n\r\nAfter these steps, Minerva should be successfully built and linked on your system. But to run the Python interpreter with Owl enabled, you have to `source ${MINERVA_ROOT}/owl_environ.sh`. This will set `PYTHONPATH` environment variable for importing modules in Python. You could also add this script to `~/.profile` or `~/.bashrc` to automate this process.\r\n\r\nMinerva is not unusual in this approach; `ssh-agent`, `pip`, `opam` and many others also work similarly. This allows a local version of the Minerva system and its dependencies, while still giving the ability to run from anywhere.\r\n\r\n## Running apps\r\n\r\nThere are two ways to use Minerva: writing C++ code or Python code. Python binding is preferred since we provide easy interaction with NumPy.\r\n\r\nWe have implemented several applications in Minerva including ImageNet training and MNIST training. After you have built Minerva, you can run both C++ and Python code.\r\n\r\nThe Python applications are located in `minerva/owl/apps`. After you have built Minerva, you can run the applications with `python {app}.py`.\r\n\r\nThe source code for C++ applicaionts are located in `minerva/apps` and there compiled executables are located in `minerva/release/apps`. You can run the executables directly.\r\n\r\nThe MNIST training data can be downloaded in: http://pan.baidu.com/s/1ntsQs0x\r\n\r\n## Writing you own app\r\n\r\nMinerva allows you to write you own code for machine learning, using a matrix interface just like Matlab or NumPy. You can use C++ or Python, whichever you prefer. The C++ and Python interface are quite similar. With Python, you can load data with NumPy and use it in Minerva, or you can convert Minerva NArrays into NumPy array and plot/print it with the tools provided in NumPy.\r\n\r\nThe NArray interface provided by Minerva is very intuitive. If you are familiar with either one of the matrix programming tools such as Matlab or NumPy, it should be very easy to get started with Minerva.\r\n\r\nMinerva allows you to use multiple GPUs at the same time. By using the `set_device` function, you can specify which device you want the operation to run on. Once set, all the operations you specify will be performed on this device.\r\n\r\nMinerva uses `lazy evaluation`, meaning that the operations are carried out only when necessary. For example, when you write `c = a + b`, the matrix addition will not be performed immediately. Instead, a dependency graph is constructed to track the dependency relationship. Once you try to evaluate the matrix c, either by printing some of its elements, or calling `c.WaitForEval()`, Minerva will lookup the dependency graph and try to carry out the operation. In this way, you can \"push\" multiple operations to different devices, and then trigger the evaluation on both devices at the same time. This is how multi-GPU programming is done in Minerva. Please refer to the code to get more details.\r\n\r\n## License and support\r\n\r\nMinerva is provided in the Apache V2 open source license.\r\n\r\nYou can use the \"issues\" tab in github to report bugs. For non-bug issues, please send up an email at minerva-support@googlegroups.com.\r\n","google":"UA-57231898-2","note":"Don't delete this file! It's used internally to help with page regeneration."}